{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Web-site Demo\n",
    "http://myweatherproject.s3-website-us-east-1.amazonaws.com/\n",
    "\n",
    "#### If viewing on github, here is a sample of the index web-page and Chicago city web-page\n",
    "[index.html](web-page_sample/index.html)  \n",
    "[chicago.html](web-page_sample/chicago.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EC2\n",
    "- Hourly cronjob (meets api data restrictions)\n",
    "- [weather_api.py](code/weather_api.py)\n",
    "\n",
    "### Weather API\n",
    "- Obtains data from Weather Underground\n",
    "- Creates a list of tuples: (city, current_weather), for speed layer\n",
    "- Ships raw data to Firehose\n",
    "\n",
    "### Speed Layer\n",
    "- Pulls in web-site HTML as String\n",
    "- Updates Current Weather for each City using Regular Expressions\n",
    "\n",
    "### E-mail\n",
    "- Sends completion e-mail that Weather API and Speed Layer are Complete\n",
    "- Indicates number of cities updated (expecting all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kinesis\n",
    "### Firehose\n",
    "- Packages raw data from all cities together into a single file\n",
    "- Ships raw data to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3\n",
    "### Raw Data\n",
    "- Stores raw data\n",
    "\n",
    "### Web Host\n",
    "- Hosts web-site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMR Spark Cluster\n",
    "### Normalize\n",
    "![](images/normalized.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- city: Data about the city\n",
    "- nearby: Nearby locations for each city\n",
    "- cityDay: Data about the city on the given date\n",
    "- weather: Weather data for the given city, date, and time\n",
    "- forecast: Forecasted weather for the city retrieved at the given date and time about the forecast date and forecast time\n",
    "\n",
    "\n",
    "- path: All S3 paths that have been loaded into the tables\n",
    "- stats: Output from analyze job discussed below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Normalize Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hourly cronjob following EC2 API job\n",
    "- [weather_normalize.py](code/weather_normalize.py)\n",
    "- Load each table from parquet\\*\n",
    "- Check S3 for any/all new files that are not in \"path\" table\n",
    "- For each new file:\n",
    " - Normalize the file's data\n",
    " - Add filepath \"source\" data for each record (track lineage)\n",
    " - Append new data to full tables\n",
    "- Enforce keys (see below)\n",
    "- Write back to parquet\n",
    "- Send Job Completion E-mail\n",
    "\n",
    "\n",
    "\\* Self-healing mechanism recreates tables from raw data if issues encountered with parquet files.  This was used during development but hasn't been encountered in production.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forecast Problem/Solution\n",
    "Problem - can't explode multiple columns  \n",
    "Solution - switch to RDD \n",
    "\n",
    "DataFrame:  \n",
    "City, Date, Time, [forecast date/times], [forecast temperatures], [forecast humidity], [ ]...  \n",
    " \n",
    "\n",
    "RDD:  \n",
    "Zip:  \n",
    "City, Date, Time, zip(forecast date/times, forecast temps, hum etc.)  \n",
    "City, Date, Time, [(dt, temp, hum, ...), (dt, temp, hum, ...), (dt, temp, hum...), ...)\n",
    "\n",
    "Reshape:  \n",
    "[(city, date, time, dt, temp, hum, ...), (city, date, time, dt, temp, hum, ...), ...]\n",
    "\n",
    "FlatMap:  \n",
    "(city, date, time, dt, temp, hum, ...)\n",
    "\n",
    "Switch Back to DF  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enforce Keys\n",
    "- I noticed that Weather Underground shipped me 2 different historical temperatures for the same city/day (they were different by 1 degree).\n",
    "- If I simply append the new data, weather underground may not live up to my keys.  \n",
    "- To enforce my keys, I will use the most recent data provided by Weather Underground for each key.\n",
    "- Because I tracked the data lineage (source) of each piece of information, I can accomplish this as follows:\n",
    "\n",
    "\n",
    "        select *\n",
    "        from\n",
    "            (select *\n",
    "            ,row_number() over(partition by city, date order by source desc) as rk\n",
    "            from cityDay2V)\n",
    "        where rk=1').drop('rk')\n",
    "        \n",
    "- I enforce keys for every table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze\n",
    "![](images/stats.png)\n",
    "- Hourly cronjob following Web Update job (we discuss it first since the previously analyzed data is used in the web update)\n",
    "- [weather_analyze.py](code/weather_analyze.py)\n",
    "- Load tables from Parquet\n",
    "- Join Actual Weather that occured back onto the Previous Forecasts that were made\n",
    "- I truncated minutes and joined to the nearest hour (reasonable since most data was between xx:00 and xx:02)\n",
    "- Calculate the number of hours between forecast and actual weather (call it \"forecast hours\")\n",
    " - For example, at 11:00 we forecast weather for 2:00, the forecast hours are 3\n",
    "- Calculate the difference between the forecast weather features and the actual weather features\n",
    "- Calculate counts, means, and standard deviations for the differences by \"forecast hours\"  \n",
    "- Write Stats to Parquet\n",
    "- Send Job Completion E-mail "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Web Update\n",
    "- Hourly cronjob following Normalize process\n",
    "- [weather_report.py](code/weather_report.py)\n",
    "- Load tables from Parquet\n",
    "- Phase 1: Preprocess DataFrames to filter on the Current Data needed and Cache the smaller tables\n",
    "- Phase 2: For each city:\n",
    " - Query all the different tables for the current data for each section of the html report\n",
    " - Create city web-page html using both strings and pandas DataFrame.to_html()\n",
    " - Create plots by joining stats with forecast, calculating confidence intervals, using DataFrame.plot(), and saving each image to S3\n",
    "- Create index and error web-pages.\n",
    "- Send Job Completion E-mail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hourly E-mails\n",
    "![](images/email.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Appendix - Big Data System Properties\n",
    "### Robustness and fault tolerance\n",
    "How does my system have this property?  \n",
    "- The system is broken down into 4 self-contained parts with e-mail notifications for the successful completion of each part.  If one of these parts break, I will immediately know which part broke and can run the code for that part directly to identify the exact error within the code.  \n",
    "- The normalize job incorporates self-healing code where if it encounters issues loading the needed parquet files, it will rebuild them from scratch from the source data.  \n",
    "- Everything downstream from the S3 raw data can be easily reconstructed or moved to another server should any part of the system go down.  \n",
    "- The enforce_keys function in the normalize process ensures that the keys for each table are respected by using the most recent data if duplicates are accidently sent from the external API.  \n",
    "\n",
    "How does my system fall short and how could it be improved?  \n",
    "- The system is dependent on the EC2 machine connecting to the data api and the firehose to stream new data into S3.  If this part of the system goes down, the weather for that timeframe would be lost forever.  \n",
    " - The successful job completion e-mail serves as one measure to limit the amount of time that would pass before this situation would be caught.  \n",
    "- The system is dependent on the weather underground api being available.  \n",
    " - Potentially, I could source weather data from a second source in case weather underground ever failed, but the development and maintenance for this may be prohibitive.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low latency reads and updates\n",
    "How does my system have this property?\n",
    "- The speed layer gets the current weather directly to the web-site immediately when it is obtained.  \n",
    "- The rest of the data is not urgent and is updated 9 minutes later (which is plenty soon enough).  \n",
    "- The web-site is hosted on S3 and loads immediately upon request.  \n",
    "\n",
    "How does my system fall short and how could it be improved?\n",
    "- The weather conditions are updated hourly.  This is a constraint of the number of api calls we are allowed to make on our budget.\n",
    " - The system could be improved by reading new weather data more frequently into our system.\n",
    "- The system could also get all of the data to the web-site with a quicker turnaround if we piped the stream directly into Spark Streaming and cut out the time delay from Firehose.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalability\n",
    "How does my system have this property?\n",
    "- The EC2 API is only performing 15 requests per hour.  There is a ton of room to scale if we paid weather underground for more requests.  \n",
    "- Firehose will automatically scale to handle an increased load.  \n",
    "- S3 is infinitely scalable for both raw data and web hosting of html pages.\n",
    "- Spark can also be scaled by easily adding more machines to increase capacity.  \n",
    "\n",
    "How does my system fall short and how could it be improved?\n",
    "- The self-healing recreation of parquet files from scratch would become more expensive if our data volume increased.\n",
    " - Instead, I would probably store the parquet files in a backup location and load from the backup if the primary load failed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization\n",
    "How does my system have this property?  \n",
    "- We store all raw data in S3 so any changes that we want to make downstream can be ran to incorporate all of the data that we have accumulated into the change.  This makes our system quite flexible and adaptible.  \n",
    "\n",
    "How does my system fall short and how could it be improved?\n",
    "- There is a short lag (few minutes) in between the api call and firehose packaging and shipping the data to S3.  This limits our ability to serve all the data downstream with no delay.  This is being overcome with the speed layer for current temperature but not for the other pieces of data.  There are potentially some other future applications that would want other real-time data that we aren't serving real-time today.  \n",
    " - As mentioned above, we could improve this by using Spark Streaming instead of firehose to essentially cut out the time delay.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensibility\n",
    "How does my system have this property?  \n",
    "- It would be very easy to update the Weather Underground API to add a new weather feature that we want to collect and the data would automatically flow into S3.  \n",
    "- The normalize process stores the data in 3NF, so the new feature would only need to belong to the single approrpriate table.  There is some development required to pull in a new feature, but the system is extensible.  \n",
    "- The web-site could also be extended to display new features.  \n",
    "- If, for example, we wanted to add additional cities, we would only have to update the EC2 weather API.  The Spark Cluster would automatically incorporate the new data.  The new city would be auto-detected and a new web-page for the city would automatically be built and incorporated!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad hoc Queries\n",
    "How does my system have this property?\n",
    "- The data tables are stored in Parquet on the cluster.  It is super easy to open a new Jupyter Notebook, point to the tables, and begin querying.  The data is already in 3NF, so it easy and obvious to join tables and create exactly what is needed.\n",
    "\n",
    "How does my system fall short and how could it be improved?\n",
    "- There were some data elements in the Raw Data json structure that didn't seem useful, and that I didn't bring into the normalized format.  I could normalize those additional features as well so they would also be available for querying in case of future use cases.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimal maintenance\n",
    "How does my system have this property?\n",
    "- Every part of the system scales very easily, with many parts that will scale automatically.  The cluster size we are using is more than sufficient to last months before we would need to add additional nodes.  \n",
    "- The data system has already run from end-to-end (producing all 4 job complete e-mails) hundreds of times without ever encountering a single error.  \n",
    "\n",
    "How does my system fall short and how could it be improved?\n",
    "- The size of the spark cluster would need to be updated periodically.  Maybe I could create a job that would measure utilization and notfiy me when we reached a certain capacity and it was time to update.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debuggability\n",
    "How does my system have this property?\n",
    "- As discussed above, the 4 discrete phases with e-mail completions makes it easy to find where the error started.  From there, it is easy to run the single failing script ad hoc and retrieve the exact line number where the error is occurring.  \n",
    "- S3 stores all raw data and the normalize process has a function that can be swapped in/out on demand to re-build from scratch whenever needed or desired.  \n",
    " - The analyze and report jobs run on all of the data, so they will continue to work even without ever caring if we reload from scratch.   \n",
    "\n",
    "How does my system fall short and how could it be improved?\n",
    "- I could write it in additional checks throughout each of the 4 phases and report out on each, so that failures would be isolated to specific portions of the code within each phase.     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsci6007]",
   "language": "python",
   "name": "conda-env-dsci6007-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
